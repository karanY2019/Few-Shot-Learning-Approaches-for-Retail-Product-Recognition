{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#from google.colab import drive \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    im = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def crop(im, r, c, target_r, target_c): return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "def center_crop(im, min_sz=None):\n",
    "    #\"\"\" Returns a center crop of an image\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    if min_sz is None: min_sz = min(r,c)\n",
    "    start_r = math.ceil((r-min_sz)/2)\n",
    "    start_c = math.ceil((c-min_sz)/2)\n",
    "    return crop(im, start_r, start_c, min_sz, min_sz)\n",
    "\n",
    "def random_crop(x, target_r, target_c):\n",
    "    #\"\"\" Returns a random crop\"\"\"\n",
    "    r,c,*_ = x.shape\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(rand_r*(r - target_r)).astype(int)\n",
    "    start_c = np.floor(rand_c*(c - target_c)).astype(int)\n",
    "    return crop(x, start_r, start_c, target_r, target_c)\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    #\"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, \n",
    "                          flags=cv2.WARP_FILL_OUTLIERS+interpolation)\n",
    "    \n",
    "def normalize(im):\n",
    "    #\"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im/255.0 - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "def apply_transforms(x, sz=(42, 42), zoom=1.05):\n",
    "    #\"\"\" Applies a random crop, rotation\"\"\"\n",
    "    sz1 = int(zoom*sz[0])\n",
    "    sz2 = int(zoom*sz[1])\n",
    "    x = cv2.resize(x, (sz1, sz2))\n",
    "    x = rotate_cv(x, np.random.uniform(-10,10))\n",
    "    x = random_crop(x, sz[1], sz[0])\n",
    "    if np.random.rand() >= .5:\n",
    "         x = np.fliplr(x).copy()\n",
    "    return x\n",
    "\n",
    "def denormalize(img):\n",
    "  imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "  return img*imagenet_stats[1] + imagenet_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "   id  label\n",
      "0   1      1\n",
      "1   2      1\n",
      "Data Stats:\n",
      "                 id         label\n",
      "count  14264.000000  14264.000000\n",
      "mean    7132.500000    124.978617\n",
      "std     4117.806455     74.287425\n",
      "min        1.000000      1.000000\n",
      "25%     3566.750000     62.000000\n",
      "50%     7132.500000    122.000000\n",
      "75%    10698.250000    182.000000\n",
      "max    14264.000000    276.000000\n"
     ]
    }
   ],
   "source": [
    "#PATH = Path('CUB_200_2011')\n",
    "PATH = Path('MixedDataset1/')\n",
    "labels = pd.read_csv(PATH/\"image_class_labels.txt\", header=None, sep=\" \")\n",
    "labels.columns = [\"id\", \"label\"]\n",
    "print(\"Labels:\")\n",
    "print(labels.head(2))\n",
    "print(\"Data Stats:\")\n",
    "print(labels.describe())\n",
    "classes = pd.read_csv(PATH/\"classes.txt\", header=None, sep=\" \")\n",
    "classes.columns = [\"id\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://debuggercafe.com/image-augmentation-using-pytorch-and-albumentations/\n",
    "import torchvision.transforms as transforms\n",
    "transform_train = torchvision.transforms.Compose(\n",
    "            [   transforms.ToPILImage(),\n",
    "                transforms.Resize([42, 42]),\n",
    "                transforms.RandomCrop(42),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(degrees=(90,270)),\n",
    "                transforms.ColorJitter(), #Randomly change the brightness, contrast and saturation of an image.\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "transform_test=torchvision.transforms.Compose(\n",
    "            [transforms.ToPILImage(),\n",
    "             transforms.Resize([84, 84]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.read_csv(PATH/\"train_test_split.txt\", header=None, sep=\" \")\n",
    "train_test.columns = [\"id\", \"is_train\"]\n",
    "\n",
    "#images = list of images and names of classes \n",
    "images = pd.read_csv(PATH/\"images.txt\", header=None, sep=\" \")\n",
    "images.columns = [\"id\", \"name\"]\n",
    "class CUB(Dataset):\n",
    "    def __init__(self, files_path, labels, train_test, image_name, train=True, \n",
    "                 transform=False):  \n",
    "        self.files_path = files_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train_test = train_test\n",
    "        self.image_name = image_name      \n",
    "        if train:\n",
    "          mask = self.train_test.is_train.values == 1       \n",
    "        else:\n",
    "          mask = self.train_test.is_train.values == 0      \n",
    "        self.filenames = self.image_name.iloc[mask]\n",
    "        self.labels = self.labels[mask]\n",
    "        self.num_files = self.labels.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    def __getitem__(self, index):\n",
    "        y = self.labels.iloc[index,1] - 1\n",
    "        \n",
    "        file_name = self.filenames.iloc[index, 1]\n",
    "        path = self.files_path/'images'/file_name\n",
    "        x = read_image(path)\n",
    "        if self.transform:\n",
    "             x = apply_transforms(x)\n",
    "             #x=transform_train(x)\n",
    "        else:\n",
    "            x = cv2.resize(x, (42,42))\n",
    "            #x = transform_test(x)\n",
    "        x = normalize(x)\n",
    "        x =  np.rollaxis(x, 2) # To meet torch's input specification(c*H*W) \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CUB(PATH, labels, train_test, images, train= True, transform= True)\n",
    "valid_dataset = CUB(PATH, labels, train_test, images, train= False, transform= False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), num_workers=0)\n",
    "print(torch.cuda.memory_allocated(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(valid_loader, 0):\n",
    "#  print(i)\n",
    "#    # get the inputs; data is a list of [inputs, labels]\n",
    "  testx, testy = data\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "#    # get the inputs; data is a list of [inputs, labels]\n",
    "#  print(i)\n",
    "  trainx, trainy = data\n",
    "\n",
    "\n",
    "#testx = torch.load(PATH/'testx.pt')\n",
    "#trainx = torch.load(PATH/'trainx.pt')\n",
    "#testy = np.load(PATH/'testy.npy')\n",
    "#trainy = np.load(PATH/'trainy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy = trainy.numpy()\n",
    "testy = testy.numpy()\n",
    "testx=testx.float()\n",
    "trainx=trainx.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader_iterator = iter(valid_loader)\n",
    "#testx, testy = next(dataloader_iterator)\n",
    "\n",
    "#dataloader_iterator = iter(train_loader)\n",
    "#trainx, trainy = next(dataloader_iterator)\n",
    "\n",
    "#trainy = trainy.numpy()\n",
    "#testy = testy.numpy()\n",
    "#testx=testx.float()\n",
    "#trainx=trainx.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6974, 3, 42, 42]),\n",
       " (6974,),\n",
       " torch.Size([7290, 3, 42, 42]),\n",
       " (7290,),\n",
       " 140,\n",
       " 136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx.shape, testy.shape, trainx.shape, trainy.shape, len(np.unique(trainy)), len(np.unique(testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Image2Vector CNN which takes image of dimension (42x42x3) and return column vector length 224\n",
    "    \"\"\"\n",
    "    def sub_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        block = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "                    torch.nn.BatchNorm2d(out_channels),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.MaxPool2d(kernel_size=2)\n",
    "                )\n",
    "        return block\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.convnet1 = self.sub_block(3,28)\n",
    "        self.convnet2 = self.sub_block(28,64)\n",
    "        self.convnet3 = self.sub_block(64,128)\n",
    "        self.convnet4 = self.sub_block(128,224)\n",
    "        self.convnet5 = self.sub_block(224,312)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet1(x)\n",
    "        x = self.convnet2(x)\n",
    "        x = self.convnet3(x)\n",
    "        x = self.convnet4(x)\n",
    "        x = self.convnet5(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNet(nn.Module):\n",
    "    def __init__(self, use_gpu=False):\n",
    "        super(PrototypicalNet, self).__init__()\n",
    "        self.f=Net()\n",
    "        #self.f=resnet18() ### ResNet \n",
    "        self.gpu = use_gpu\n",
    "        if self.gpu:\n",
    "            self.f = self.f.cuda()\n",
    "    \n",
    "    def forward(self, datax, datay, Ns,Nc, Nq, total_classes):\n",
    "        \"\"\"\n",
    "        Implementation of one episode in Prototypical Net\n",
    "        datax: Training images\n",
    "        datay: Corresponding labels of datax\n",
    "        Nc: Number  of classes per episode\n",
    "        Ns: Number of support data per class\n",
    "        Nq:  Number of query data per class\n",
    "        total_classes: Total classes in training set\n",
    "        \"\"\"\n",
    "        k = total_classes.shape[0]\n",
    "        K = np.random.choice(total_classes, Nc, replace=False)\n",
    "        Query_x = torch.Tensor()\n",
    "        if(self.gpu):\n",
    "            Query_x = Query_x.cuda()\n",
    "        Query_y = []\n",
    "        Query_y_count = []\n",
    "        centroid_per_class  = {}\n",
    "        class_label = {}\n",
    "        label_encoding = 0\n",
    "        for cls in K:\n",
    "            S_cls, Q_cls = self.random_sample_cls(datax, datay, Ns, Nq, cls)\n",
    "            centroid_per_class[cls] = self.get_centroid(S_cls, Ns)\n",
    "            class_label[cls] = label_encoding\n",
    "            label_encoding += 1\n",
    "            Query_x = torch.cat((Query_x, Q_cls), 0) # Joining all the query set together\n",
    "            Query_y += [cls]\n",
    "            Query_y_count += [Q_cls.shape[0]]\n",
    "        Query_y, Query_y_labels = self.get_query_y(Query_y, Query_y_count, class_label)\n",
    "        Query_x = self.get_query_x(Query_x, centroid_per_class, Query_y_labels)\n",
    "        return Query_x, Query_y\n",
    "    \n",
    "    def random_sample_cls(self, datax, datay, Ns, Nq, cls):\n",
    "        \"\"\"\n",
    "        Randomly samples Ns examples as support set and Nq as Query set\n",
    "        \"\"\"\n",
    "        data = datax[(datay == cls).nonzero()]\n",
    "        perm = torch.randperm(data.shape[0])\n",
    "        idx = perm[:Ns]\n",
    "        S_cls = data[idx]\n",
    "        idx = perm[Ns : Ns+Nq]\n",
    "        Q_cls = data[idx]\n",
    "        if self.gpu:\n",
    "            S_cls = S_cls.cuda()\n",
    "            Q_cls = Q_cls.cuda()\n",
    "        return S_cls, Q_cls  #return support set and Query set\n",
    "    \n",
    "    def get_centroid(self, S_cls, Nc):\n",
    "        \"\"\"\n",
    "        Returns a centroid vector of support set for a class\n",
    "        \"\"\"\n",
    "        #use f *** !!!\n",
    "        return torch.sum(self.f(S_cls), 0).unsqueeze(1).transpose(0,1) / Nc\n",
    "    \n",
    "    def get_query_y(self, Qy, Qyc, class_label):\n",
    "        \"\"\"\n",
    "        Returns labeled representation of classes of Query set and a list of labels.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        m = len(Qy)\n",
    "        for i in range(m):\n",
    "            labels += [Qy[i]] * Qyc[i]\n",
    "        labels = np.array(labels).reshape(len(labels), 1)\n",
    "        label_encoder = LabelEncoder()\n",
    "        Query_y = torch.Tensor(label_encoder.fit_transform(labels).astype(int)).long()\n",
    "        if self.gpu:\n",
    "            Query_y = Query_y.cuda()\n",
    "        Query_y_labels = np.unique(labels)\n",
    "        return Query_y, Query_y_labels\n",
    "    \n",
    "    def get_centroid_matrix(self, centroid_per_class, Query_y_labels):\n",
    "        \"\"\"\n",
    "        Returns the centroid matrix where each column is a centroid of a class.\n",
    "        \"\"\"\n",
    "        centroid_matrix = torch.Tensor()\n",
    "        if(self.gpu):\n",
    "            centroid_matrix = centroid_matrix.cuda()\n",
    "        for label in Query_y_labels:\n",
    "            centroid_matrix = torch.cat((centroid_matrix, centroid_per_class[label]))\n",
    "        if self.gpu:\n",
    "            centroid_matrix = centroid_matrix.cuda()\n",
    "        return centroid_matrix\n",
    "    \n",
    "    def get_query_x(self, Query_x, centroid_per_class, Query_y_labels):\n",
    "        \"\"\"\n",
    "        Returns distance matrix from each Query image to each centroid.\n",
    "        \"\"\"\n",
    "        centroid_matrix = self.get_centroid_matrix(centroid_per_class, Query_y_labels)\n",
    "        Query_x = self.f(Query_x)  #use f *****\n",
    "        m = Query_x.size(0)\n",
    "        n = centroid_matrix.size(0)\n",
    "        # The below expressions expand both the matrices such that they become compatible to each other in order to caclulate L2 distance.\n",
    "        centroid_matrix = centroid_matrix.expand(m, centroid_matrix.size(0), centroid_matrix.size(1)) # Expanding centroid matrix to \"m\".\n",
    "        Query_matrix = Query_x.expand(n, Query_x.size(0), Query_x.size(1)).transpose(0,1) # Expanding Query matrix \"n\" times\n",
    "        Qx = torch.pairwise_distance(centroid_matrix.transpose(1,2), Query_matrix.transpose(1,2))\n",
    "        return Qx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protonet = PrototypicalNet(use_gpu=use_gpu)#use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protonet = PrototypicalNet(use_gpu=use_gpu)#use_gpu=use_gpu)\n",
    "optimizer = optim.SGD(protonet.parameters(), lr = 0.01, momentum=0.99)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223773184\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1447034880"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385875968"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(datax, datay, Ns,Nc, Nq):\n",
    "    optimizer.zero_grad() #if need this for pretrained model ???\n",
    "    Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n",
    "    pred = torch.log_softmax(Qx, dim=-1)\n",
    "    loss = F.nll_loss(pred, Qy)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #lr_scheduler.step()\n",
    "    acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "save_path='save/proto_mix'\n",
    "def save_model(name):\n",
    "    torch.save(protonet.state_dict(), osp.join(save_path, name + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 8000 #3000+\n",
    "frame_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-07 19:05:57.843223 Frame Number: 1 Frame Loss:  0.24372451782226562 Frame Accuracy: 92.82457275390625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3a475507ddd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrainx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mframe_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-7ac9bfdf2ab7>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(datax, datay, Ns, Nc, Nq)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#lr_scheduler.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trlog = {}\n",
    "    #trlog['args'] = vars(args)\n",
    "trlog['train_loss'] = []\n",
    "#trlog['val_loss'] = []\n",
    "trlog['train_acc'] = []\n",
    "#trlog['val_acc'] = []\n",
    "trlog['max_acc'] = 0.0\n",
    "    \n",
    "frame_loss = 0\n",
    "frame_acc = 0\n",
    "losses = []\n",
    "\n",
    "#model=protonet\n",
    "#load='./save/proto_mix/max-acc.pth'  \n",
    "#model.load_state_dict(torch.load(load))\n",
    "\n",
    "for i in range(num_episode):\n",
    "    \n",
    "    if use_gpu:\n",
    "        trainx = trainx.cuda() \n",
    "    loss, acc = train_step(trainx, trainy, 5, 20, 15)\n",
    "    \n",
    "    frame_loss += loss.data\n",
    "    frame_acc += acc.data\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if( (i+1) % frame_size == 0):\n",
    "        print(datetime.datetime.now(),\"Frame Number:\", ((i+1) // frame_size), 'Frame Loss: ', frame_loss.data.cpu().numpy().tolist()/ frame_size, 'Frame Accuracy:', (frame_acc.data.cpu().numpy().tolist() * 100) / frame_size)\n",
    "        if frame_acc > trlog['max_acc']:\n",
    "              trlog['max_acc'] = frame_acc\n",
    "        save_model('max-acc')\n",
    "            \n",
    "        frame_loss = 0\n",
    "        frame_acc = 0\n",
    "    \n",
    "                \n",
    "\n",
    "    #trlog['train_loss'].append(tl)\n",
    "    #trlog['train_acc'].append(ta)\n",
    "        #trlog['val_loss'].append(vl)\n",
    "        #trlog['val_acc'].append(va)\n",
    "\n",
    "    torch.save(trlog, osp.join(save_path, 'trlog'))\n",
    "\n",
    "    save_model('epoch-last')\n",
    "        \n",
    "    save_frame=1000\n",
    "    if i % save_frame == 0:\n",
    "        save_model('epoch-{}'.format(i))\n",
    "    \n",
    "    del acc, loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385875968"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(datax, datay, Ns,Nc, Nq):\n",
    "    #model=protonet\n",
    "    with torch.no_grad():\n",
    "        Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n",
    "        pred = torch.log_softmax(Qx, dim=-1)\n",
    "        loss = F.nll_loss(pred, Qy)\n",
    "        acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episode = 1000 #2000+\n",
    "frame_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-07 19:14:53.137457 Frame Number: 1 Frame Loss:  1.767197265625 Frame Accuracy: 409.73577880859375\n",
      "2020-05-07 19:14:56.026379 Frame Number: 2 Frame Loss:  3.01162109375 Frame Accuracy: 469.5752258300781\n",
      "2020-05-07 19:14:58.911997 Frame Number: 3 Frame Loss:  4.1874560546875 Frame Accuracy: 531.5953979492188\n",
      "2020-05-07 19:15:01.789832 Frame Number: 4 Frame Loss:  5.373756713867188 Frame Accuracy: 593.8726806640625\n",
      "2020-05-07 19:15:04.534697 Frame Number: 5 Frame Loss:  6.604613037109375 Frame Accuracy: 654.0866088867188\n",
      "2020-05-07 19:15:07.355679 Frame Number: 6 Frame Loss:  7.8074407958984375 Frame Accuracy: 715.1099243164062\n",
      "2020-05-07 19:15:10.176715 Frame Number: 7 Frame Loss:  8.964473266601562 Frame Accuracy: 777.2021484375\n",
      "2020-05-07 19:15:13.053482 Frame Number: 8 Frame Loss:  10.1263134765625 Frame Accuracy: 839.2717895507812\n",
      "2020-05-07 19:15:15.941803 Frame Number: 9 Frame Loss:  11.322021484375 Frame Accuracy: 899.3596801757812\n",
      "2020-05-07 19:15:18.771217 Frame Number: 10 Frame Loss:  12.57939697265625 Frame Accuracy: 959.185546875\n",
      "Avg Loss:  1.1937476806640626 Avg Accuracy: 61.246588134765624\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "#protonet.eval()\n",
    "test_losses = []\n",
    "#model=protonet\n",
    "#load='./save/proto_mix/max-acc.pth'  \n",
    "#model.load_state_dict(torch.load(load))\n",
    "\n",
    "for i in range(num_test_episode):\n",
    "    #if use_gpu:\n",
    "        #testx = testx.cuda() \n",
    "    loss, acc = test_step(testx, testy, 5, 5, 15)\n",
    "    frame_loss += loss.data\n",
    "    frame_acc += acc.data\n",
    "    \n",
    "    if( (i+1) % frame_size == 0):\n",
    "        print(datetime.datetime.now(),\"Frame Number:\", ((i+1) // frame_size), 'Frame Loss: ', frame_loss.data.cpu().numpy().tolist()/ frame_size, 'Frame Accuracy:', (frame_acc.data.cpu().numpy().tolist() * 100) / frame_size)\n",
    "    avg_loss += loss.data\n",
    "    avg_acc += acc.data\n",
    "    #test_losses.append(loss)\n",
    "    #del acc, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print('Avg Loss: ', avg_loss.data.cpu().numpy().tolist() / num_test_episode , 'Avg Accuracy:', (avg_acc.data.cpu().numpy().tolist() * 100) / num_test_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2000: 61.228% test acc with 70.5% training acc\n",
    "\n",
    "#test2000: 60.7   train 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet : 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\" Losses\")\n",
    "plt.plot(losses,label=\"Train\")\n",
    "plt.plot(test_losses,label=\"Test\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small net: D=64 , acc: 50%\n",
    "Big net:resize-42, D=224, acc: 58%\n",
    "        with color-based+rotation augmentation: ..., take longer to train\n",
    "TDDO: resize:84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
