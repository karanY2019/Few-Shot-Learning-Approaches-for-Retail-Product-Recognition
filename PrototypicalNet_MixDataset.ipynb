{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#from google.colab import drive \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    im = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def crop(im, r, c, target_r, target_c): return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "def center_crop(im, min_sz=None):\n",
    "    #\"\"\" Returns a center crop of an image\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    if min_sz is None: min_sz = min(r,c)\n",
    "    start_r = math.ceil((r-min_sz)/2)\n",
    "    start_c = math.ceil((c-min_sz)/2)\n",
    "    return crop(im, start_r, start_c, min_sz, min_sz)\n",
    "\n",
    "def random_crop(x, target_r, target_c):\n",
    "    #\"\"\" Returns a random crop\"\"\"\n",
    "    r,c,*_ = x.shape\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(rand_r*(r - target_r)).astype(int)\n",
    "    start_c = np.floor(rand_c*(c - target_c)).astype(int)\n",
    "    return crop(x, start_r, start_c, target_r, target_c)\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    #\"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, \n",
    "                          flags=cv2.WARP_FILL_OUTLIERS+interpolation)\n",
    "    \n",
    "def normalize(im):\n",
    "    #\"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im/255.0 - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "def apply_transforms(x, sz=(84, 84), zoom=1.05):\n",
    "    #\"\"\" Applies a random crop, rotation\"\"\"\n",
    "    sz1 = int(zoom*sz[0])\n",
    "    sz2 = int(zoom*sz[1])\n",
    "    x = cv2.resize(x, (sz1, sz2))\n",
    "    x = rotate_cv(x, np.random.uniform(-10,10))\n",
    "    x = random_crop(x, sz[1], sz[0])\n",
    "    if np.random.rand() >= .5:\n",
    "         x = np.fliplr(x).copy()\n",
    "    return x\n",
    "\n",
    "def denormalize(img):\n",
    "  imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "  return img*imagenet_stats[1] + imagenet_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "   id  label\n",
      "0   1      1\n",
      "1   2      1\n",
      "Data Stats:\n",
      "                 id         label\n",
      "count  14264.000000  14264.000000\n",
      "mean    7132.500000    124.978617\n",
      "std     4117.806455     74.287425\n",
      "min        1.000000      1.000000\n",
      "25%     3566.750000     62.000000\n",
      "50%     7132.500000    122.000000\n",
      "75%    10698.250000    182.000000\n",
      "max    14264.000000    276.000000\n"
     ]
    }
   ],
   "source": [
    "#PATH = Path('CUB_200_2011')\n",
    "PATH = Path('MixedDataset1/')\n",
    "labels = pd.read_csv(PATH/\"image_class_labels.txt\", header=None, sep=\" \")\n",
    "labels.columns = [\"id\", \"label\"]\n",
    "print(\"Labels:\")\n",
    "print(labels.head(2))\n",
    "print(\"Data Stats:\")\n",
    "print(labels.describe())\n",
    "classes = pd.read_csv(PATH/\"classes.txt\", header=None, sep=\" \")\n",
    "classes.columns = [\"id\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.read_csv(PATH/\"train_test_split.txt\", header=None, sep=\" \")\n",
    "train_test.columns = [\"id\", \"is_train\"]\n",
    "\n",
    "#images = list of images and names of classes \n",
    "images = pd.read_csv(PATH/\"images.txt\", header=None, sep=\" \")\n",
    "images.columns = [\"id\", \"name\"]\n",
    "class CUB(Dataset):\n",
    "    def __init__(self, files_path, labels, train_test, image_name, train=True, \n",
    "                 transform=False):  \n",
    "        self.files_path = files_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.train_test = train_test\n",
    "        self.image_name = image_name      \n",
    "        if train:\n",
    "          mask = self.train_test.is_train.values == 1       \n",
    "        else:\n",
    "          mask = self.train_test.is_train.values == 0      \n",
    "        self.filenames = self.image_name.iloc[mask]\n",
    "        self.labels = self.labels[mask]\n",
    "        self.num_files = self.labels.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "    def __getitem__(self, index):\n",
    "        y = self.labels.iloc[index,1] - 1\n",
    "        \n",
    "        file_name = self.filenames.iloc[index, 1]\n",
    "        path = self.files_path/'images'/file_name\n",
    "        x = read_image(path)\n",
    "        if self.transform:\n",
    "             x = apply_transforms(x)\n",
    "             #x=transform_train(x)\n",
    "        else:\n",
    "            x = cv2.resize(x, (84,84))\n",
    "            #x = transform_test(x)\n",
    "        x = normalize(x)\n",
    "        x =  np.rollaxis(x, 2) # To meet torch's input specification(c*H*W) \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CUB(PATH, labels, train_test, images, train= True, transform= True)\n",
    "valid_dataset = CUB(PATH, labels, train_test, images, train= False, transform= False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), num_workers=0)\n",
    "print(torch.cuda.memory_allocated(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(valid_loader, 0):\n",
    "#  print(i)\n",
    "#    # get the inputs; data is a list of [inputs, labels]\n",
    "  testx, testy = data\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "#    # get the inputs; data is a list of [inputs, labels]\n",
    "#  print(i)\n",
    "  trainx, trainy = data\n",
    "\n",
    "\n",
    "#testx = torch.load(PATH/'testx.pt')\n",
    "#trainx = torch.load(PATH/'trainx.pt')\n",
    "#testy = np.load(PATH/'testy.npy')\n",
    "#trainy = np.load(PATH/'trainy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy = trainy.numpy()\n",
    "testy = testy.numpy()\n",
    "testx=testx.float()\n",
    "trainx=trainx.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6974, 3, 84, 84]),\n",
       " (6974,),\n",
       " torch.Size([7290, 3, 84, 84]),\n",
       " (7290,),\n",
       " 140,\n",
       " 136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx.shape, testy.shape, trainx.shape, trainy.shape, len(np.unique(trainy)), len(np.unique(testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Image2Vector CNN which takes image of dimension (42x42x3) and return column vector length 224\n",
    "    \"\"\"\n",
    "    def sub_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        block = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "                    torch.nn.BatchNorm2d(out_channels),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.MaxPool2d(kernel_size=2)\n",
    "                )\n",
    "        return block\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.convnet1 = self.sub_block(3,28)\n",
    "        self.convnet2 = self.sub_block(28,64)\n",
    "        self.convnet3 = self.sub_block(64,128)\n",
    "        self.convnet4 = self.sub_block(128,224)\n",
    "        self.convnet5 = self.sub_block(224,512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet1(x)\n",
    "        x = self.convnet2(x)\n",
    "        x = self.convnet3(x)\n",
    "        x = self.convnet4(x)\n",
    "        x = self.convnet5(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNet(nn.Module):\n",
    "    def __init__(self, use_gpu=False):\n",
    "        super(PrototypicalNet, self).__init__()\n",
    "        self.f=Net()\n",
    "        #self.f=resnet18() ### ResNet \n",
    "        self.gpu = use_gpu\n",
    "        if self.gpu:\n",
    "            self.f = self.f.cuda()\n",
    "    \n",
    "    def forward(self, datax, datay, Ns,Nc, Nq, total_classes):\n",
    "        \"\"\"\n",
    "        Implementation of one episode in Prototypical Net\n",
    "        datax: Training images\n",
    "        datay: Corresponding labels of datax\n",
    "        Nc: Number  of classes per episode\n",
    "        Ns: Number of support data per class\n",
    "        Nq:  Number of query data per class\n",
    "        total_classes: Total classes in training set\n",
    "        \"\"\"\n",
    "        k = total_classes.shape[0]\n",
    "        K = np.random.choice(total_classes, Nc, replace=False)\n",
    "        Query_x = torch.Tensor()\n",
    "        if(self.gpu):\n",
    "            Query_x = Query_x.cuda()\n",
    "        Query_y = []\n",
    "        Query_y_count = []\n",
    "        centroid_per_class  = {}\n",
    "        class_label = {}\n",
    "        label_encoding = 0\n",
    "        for cls in K:\n",
    "            S_cls, Q_cls = self.random_sample_cls(datax, datay, Ns, Nq, cls)\n",
    "            centroid_per_class[cls] = self.get_centroid(S_cls, Ns)\n",
    "            class_label[cls] = label_encoding\n",
    "            label_encoding += 1\n",
    "            Query_x = torch.cat((Query_x, Q_cls), 0) # Joining all the query set together\n",
    "            Query_y += [cls]\n",
    "            Query_y_count += [Q_cls.shape[0]]\n",
    "        Query_y, Query_y_labels = self.get_query_y(Query_y, Query_y_count, class_label)\n",
    "        Query_x = self.get_query_x(Query_x, centroid_per_class, Query_y_labels)\n",
    "        return Query_x, Query_y\n",
    "    \n",
    "    def random_sample_cls(self, datax, datay, Ns, Nq, cls):\n",
    "        \"\"\"\n",
    "        Randomly samples Ns examples as support set and Nq as Query set\n",
    "        \"\"\"\n",
    "        data = datax[(datay == cls).nonzero()]\n",
    "        perm = torch.randperm(data.shape[0])\n",
    "        idx = perm[:Ns]\n",
    "        S_cls = data[idx]\n",
    "        idx = perm[Ns : Ns+Nq]\n",
    "        Q_cls = data[idx]\n",
    "        if self.gpu:\n",
    "            S_cls = S_cls.cuda()\n",
    "            Q_cls = Q_cls.cuda()\n",
    "        return S_cls, Q_cls  #return support set and Query set\n",
    "    \n",
    "    def get_centroid(self, S_cls, Nc):\n",
    "        \"\"\"\n",
    "        Returns a centroid vector of support set for a class\n",
    "        \"\"\"\n",
    "        #use f *** !!!\n",
    "        return torch.sum(self.f(S_cls), 0).unsqueeze(1).transpose(0,1) / Nc\n",
    "    \n",
    "    def get_query_y(self, Qy, Qyc, class_label):\n",
    "        \"\"\"\n",
    "        Returns labeled representation of classes of Query set and a list of labels.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        m = len(Qy)\n",
    "        for i in range(m):\n",
    "            labels += [Qy[i]] * Qyc[i]\n",
    "        labels = np.array(labels).reshape(len(labels), 1)\n",
    "        label_encoder = LabelEncoder()\n",
    "        Query_y = torch.Tensor(label_encoder.fit_transform(labels).astype(int)).long()\n",
    "        if self.gpu:\n",
    "            Query_y = Query_y.cuda()\n",
    "        Query_y_labels = np.unique(labels)\n",
    "        return Query_y, Query_y_labels\n",
    "    \n",
    "    def get_centroid_matrix(self, centroid_per_class, Query_y_labels):\n",
    "        \"\"\"\n",
    "        Returns the centroid matrix where each column is a centroid of a class.\n",
    "        \"\"\"\n",
    "        centroid_matrix = torch.Tensor()\n",
    "        if(self.gpu):\n",
    "            centroid_matrix = centroid_matrix.cuda()\n",
    "        for label in Query_y_labels:\n",
    "            centroid_matrix = torch.cat((centroid_matrix, centroid_per_class[label]))\n",
    "        if self.gpu:\n",
    "            centroid_matrix = centroid_matrix.cuda()\n",
    "        return centroid_matrix\n",
    "    \n",
    "    def get_query_x(self, Query_x, centroid_per_class, Query_y_labels):\n",
    "        \"\"\"\n",
    "        Returns distance matrix from each Query image to each centroid.\n",
    "        \"\"\"\n",
    "        centroid_matrix = self.get_centroid_matrix(centroid_per_class, Query_y_labels)\n",
    "        Query_x = self.f(Query_x)  #use f *****\n",
    "        m = Query_x.size(0)\n",
    "        n = centroid_matrix.size(0)\n",
    "        # The below expressions expand both the matrices such that they become compatible to each other in order to caclulate L2 distance.\n",
    "        centroid_matrix = centroid_matrix.expand(m, centroid_matrix.size(0), centroid_matrix.size(1)) # Expanding centroid matrix to \"m\".\n",
    "        Query_matrix = Query_x.expand(n, Query_x.size(0), Query_x.size(1)).transpose(0,1) # Expanding Query matrix \"n\" times\n",
    "        Qx = torch.pairwise_distance(centroid_matrix.transpose(1,2), Query_matrix.transpose(1,2))\n",
    "        return Qx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protonet = PrototypicalNet(use_gpu=use_gpu)#use_gpu=use_gpu)\n",
    "optimizer = optim.SGD(protonet.parameters(), lr = 0.01, momentum=0.99)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5549056\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated(device=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(datax, datay, Ns,Nc, Nq):\n",
    "    optimizer.zero_grad() #if need this for pretrained model ???\n",
    "    Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n",
    "    pred = torch.log_softmax(Qx, dim=-1)\n",
    "    loss = F.nll_loss(pred, Qy)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #lr_scheduler.step()\n",
    "    acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "save_path='save/proto_mix'\n",
    "def save_model(name):\n",
    "    torch.save(protonet.state_dict(), osp.join(save_path, name + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 8000 #3000+\n",
    "frame_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-07 20:56:00.521148 Frame Number: 1 Frame Loss:  0.6181955337524414 Frame Accuracy: 77.74716186523438\n",
      "2020-05-07 20:56:35.537738 Frame Number: 2 Frame Loss:  0.6134779357910156 Frame Accuracy: 77.83401489257812\n",
      "2020-05-07 20:57:10.577304 Frame Number: 3 Frame Loss:  0.6125383377075195 Frame Accuracy: 77.95117950439453\n",
      "2020-05-07 20:57:45.092152 Frame Number: 4 Frame Loss:  0.5603770065307617 Frame Accuracy: 79.31588745117188\n",
      "2020-05-07 20:58:19.761719 Frame Number: 5 Frame Loss:  0.5337982559204102 Frame Accuracy: 81.04470825195312\n",
      "2020-05-07 20:58:54.507792 Frame Number: 6 Frame Loss:  0.5254975891113282 Frame Accuracy: 80.80546569824219\n",
      "2020-05-07 20:59:29.175646 Frame Number: 7 Frame Loss:  0.5179866790771485 Frame Accuracy: 81.16112518310547\n",
      "2020-05-07 21:00:03.916550 Frame Number: 8 Frame Loss:  0.49731319427490234 Frame Accuracy: 82.16382598876953\n",
      "2020-05-07 21:00:38.689264 Frame Number: 9 Frame Loss:  0.48128337860107423 Frame Accuracy: 82.44027709960938\n",
      "2020-05-07 21:01:13.329379 Frame Number: 10 Frame Loss:  0.46743621826171877 Frame Accuracy: 83.30577850341797\n",
      "2020-05-07 21:01:47.225366 Frame Number: 11 Frame Loss:  0.47628536224365237 Frame Accuracy: 82.6060562133789\n",
      "2020-05-07 21:02:20.606614 Frame Number: 12 Frame Loss:  0.4498639678955078 Frame Accuracy: 83.48445892333984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-50b4219574d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrainx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mframe_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-7ac9bfdf2ab7>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(datax, datay, Ns, Nc, Nq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#if need this for pretrained model ???\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mQx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQy\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mprotonet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-a890d0fe9790>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, datax, datay, Ns, Nc, Nq, total_classes)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mclass_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mlabel_encoding\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mQuery_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuery_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Joining all the query set together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mQuery_y\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mQuery_y_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mQ_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trlog = {}\n",
    "    #trlog['args'] = vars(args)\n",
    "trlog['train_loss'] = []\n",
    "#trlog['val_loss'] = []\n",
    "trlog['train_acc'] = []\n",
    "#trlog['val_acc'] = []\n",
    "trlog['max_acc'] = 0.0\n",
    "    \n",
    "frame_loss = 0\n",
    "frame_acc = 0\n",
    "losses = []\n",
    "\n",
    "#model=protonet\n",
    "#load='./save/proto_mix/max-acc.pth'  \n",
    "#model.load_state_dict(torch.load(load))\n",
    "\n",
    "for i in range(num_episode):\n",
    "    \n",
    "    if use_gpu:\n",
    "        trainx = trainx.cuda() \n",
    "    loss, acc = train_step(trainx, trainy, 5, 20, 15)\n",
    "    \n",
    "    frame_loss += loss.data\n",
    "    frame_acc += acc.data\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if( (i+1) % frame_size == 0):\n",
    "        print(datetime.datetime.now(),\"Frame Number:\", ((i+1) // frame_size), 'Frame Loss: ', frame_loss.data.cpu().numpy().tolist()/ frame_size, 'Frame Accuracy:', (frame_acc.data.cpu().numpy().tolist() * 100) / frame_size)\n",
    "        if frame_acc > trlog['max_acc']:\n",
    "              trlog['max_acc'] = frame_acc\n",
    "        save_model('max-acc')\n",
    "            \n",
    "        frame_loss = 0\n",
    "        frame_acc = 0\n",
    "    \n",
    "    #trlog['train_loss'].append(tl)\n",
    "    #trlog['train_acc'].append(ta)\n",
    "        #trlog['val_loss'].append(vl)\n",
    "        #trlog['val_acc'].append(va)\n",
    "\n",
    "    torch.save(trlog, osp.join(save_path, 'trlog'))\n",
    "\n",
    "    save_model('epoch-last')\n",
    "        \n",
    "    save_frame=1000\n",
    "    if i % save_frame == 0:\n",
    "        save_model('epoch-{}'.format(i))\n",
    "    \n",
    "    del acc, loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1778384896"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(datax, datay, Ns,Nc, Nq):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Qx, Qy= protonet(datax, datay, Ns, Nc, Nq, np.unique(datay))\n",
    "        pred = torch.log_softmax(Qx, dim=-1)\n",
    "        loss = F.nll_loss(pred, Qy)\n",
    "        acc = torch.mean((torch.argmax(pred, 1) == Qy).float())\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episode = 2000 #2000+\n",
    "frame_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-07 21:02:58.770087 Frame Number: 1 Frame Loss:  1.0952796173095702 Frame Accuracy: 62.1792106628418\n",
      "2020-05-07 21:03:04.020371 Frame Number: 2 Frame Loss:  1.101473617553711 Frame Accuracy: 61.78257369995117\n",
      "2020-05-07 21:03:09.170598 Frame Number: 3 Frame Loss:  1.055647506713867 Frame Accuracy: 61.63201141357422\n",
      "2020-05-07 21:03:14.343756 Frame Number: 4 Frame Loss:  1.0524270629882813 Frame Accuracy: 62.5540771484375\n",
      "2020-05-07 21:03:19.641794 Frame Number: 5 Frame Loss:  1.0766129302978515 Frame Accuracy: 62.446495056152344\n",
      "2020-05-07 21:03:24.795557 Frame Number: 6 Frame Loss:  1.09935302734375 Frame Accuracy: 60.34991455078125\n",
      "2020-05-07 21:03:30.091776 Frame Number: 7 Frame Loss:  1.1049430084228515 Frame Accuracy: 62.01080322265625\n",
      "2020-05-07 21:03:35.341509 Frame Number: 8 Frame Loss:  1.0502513885498046 Frame Accuracy: 63.14967346191406\n",
      "2020-05-07 21:03:40.622073 Frame Number: 9 Frame Loss:  1.2034375762939453 Frame Accuracy: 58.069496154785156\n",
      "2020-05-07 21:03:45.893922 Frame Number: 10 Frame Loss:  1.0033378601074219 Frame Accuracy: 64.66426849365234\n",
      "2020-05-07 21:03:51.232996 Frame Number: 11 Frame Loss:  1.1026349639892579 Frame Accuracy: 61.26810073852539\n",
      "2020-05-07 21:03:56.529713 Frame Number: 12 Frame Loss:  1.0495120239257814 Frame Accuracy: 63.20548629760742\n",
      "2020-05-07 21:04:01.740630 Frame Number: 13 Frame Loss:  0.9903201293945313 Frame Accuracy: 64.13916778564453\n",
      "2020-05-07 21:04:07.021670 Frame Number: 14 Frame Loss:  1.0706753540039062 Frame Accuracy: 62.148075103759766\n",
      "2020-05-07 21:04:12.392949 Frame Number: 15 Frame Loss:  1.042007293701172 Frame Accuracy: 63.02360153198242\n",
      "2020-05-07 21:04:17.746499 Frame Number: 16 Frame Loss:  1.0367731475830078 Frame Accuracy: 63.14055633544922\n",
      "2020-05-07 21:04:23.090783 Frame Number: 17 Frame Loss:  1.0611939239501953 Frame Accuracy: 62.800838470458984\n",
      "2020-05-07 21:04:28.438570 Frame Number: 18 Frame Loss:  1.0990374755859376 Frame Accuracy: 60.764522552490234\n",
      "2020-05-07 21:04:33.834309 Frame Number: 19 Frame Loss:  1.0928684234619142 Frame Accuracy: 62.601593017578125\n",
      "2020-05-07 21:04:39.179464 Frame Number: 20 Frame Loss:  1.0518099212646483 Frame Accuracy: 62.570343017578125\n",
      "Avg Loss:  1.071980224609375 Avg Accuracy: 62.225067138671875\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "#protonet.eval()\n",
    "test_losses = []\n",
    "#model=protonet\n",
    "#load='./save/proto_mix/max-acc.pth'  \n",
    "#model.load_state_dict(torch.load(load))\n",
    "frame_loss = 0\n",
    "frame_acc = 0\n",
    "for i in range(num_test_episode):\n",
    "    #if use_gpu:\n",
    "        #testx = testx.cuda() \n",
    "    loss, acc = test_step(testx, testy, 5, 5, 15)\n",
    "    frame_loss += loss.data\n",
    "    frame_acc += acc.data\n",
    "    \n",
    "    if( (i+1) % frame_size == 0):\n",
    "        print(datetime.datetime.now(),\"Frame Number:\", ((i+1) // frame_size), 'Frame Loss: ', frame_loss.data.cpu().numpy().tolist()/ frame_size, 'Frame Accuracy:', (frame_acc.data.cpu().numpy().tolist() * 100) / frame_size)\n",
    "        frame_loss = 0\n",
    "        frame_acc = 0\n",
    "    avg_loss += loss.data\n",
    "    avg_acc += acc.data\n",
    "    test_losses.append(loss)\n",
    "    #del acc, loss\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print('Avg Loss: ', avg_loss.data.cpu().numpy().tolist() / num_test_episode , 'Avg Accuracy:', (avg_acc.data.cpu().numpy().tolist() * 100) / num_test_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2000: 61.228% test acc with 70.5% training acc\n",
    "\n",
    "#test2000: 60.7   train 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\" Losses\")\n",
    "plt.plot(losses,label=\"Train\")\n",
    "plt.plot(test_losses,label=\"Test\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
